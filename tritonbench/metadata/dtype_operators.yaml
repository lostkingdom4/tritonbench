# =================================================================
# This file is generated by benchmarks/gen_metadata/run.py
# List of operators and their default dtype
# =================================================================
addmm: fp16
bf16xint16_gemm: bypass
blackwell_attentions: bf16
cross_entropy: bypass
decoding_attention: bf16
embedding: bypass
flash_attention: bf16
flex_attention: bf16
fp32_to_mx4: bypass
fp8_attention: fp8
fp8_fused_quant_gemm_rowwise: fp8
fp8_gemm: fp8
fp8_gemm_blockwise: fp8
fp8_gemm_rowwise: fp8
fp8_gemm_rowwise_grouped: fp8
fused_linear_cross_entropy: bypass
fused_linear_jsd: bypass
gather_gemv: bypass
gdn_fwd_h: bypass
gdpa: bf16
geglu: bypass
gemm: fp16
grouped_gemm: bf16
int4_gemm: bypass
jagged_layer_norm: fp32
jagged_mean: fp32
jagged_softmax: fp32
jagged_sum: fp32
jsd: bypass
kl_div: bypass
launch_latency: bypass
layer_norm: bypass
low_mem_dropout: bypass
mamba2_chunk_scan: fp16
mamba2_chunk_state: fp16
mixed_gemm: bf16
mx4_to_fp32: bypass
ragged_attention: bf16
rms_norm: bypass
rope: bypass
softmax: fp16
sum: bypass
swiglu: bypass
template_attention: bypass
test_op: bypass
vector_add: bypass
vector_exp: bypass
welford: bypass
