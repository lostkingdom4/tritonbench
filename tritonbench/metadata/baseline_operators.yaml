# =================================================================
# This file is generated by benchmarks/gen_metadata/run.py
# List of operators that have baseline benchmark for accuracy and speedup.
# =================================================================
addmm: aten_addmm
bf16xint16_gemm: bf16xbf16
cross_entropy: cross_entropy_loss
decoding_attention: triton_splitk
embedding: torch_embedding
flash_attention: aten
flex_attention: eager
fp32_to_mx4: fbgemm_fp32_to_mx4
fp8_attention: triton_flash_v2
fp8_gemm: torch_fp8_gemm
fp8_gemm_blockwise: _cutlass
fp8_gemm_rowwise: _cutlass_or_ck
fp8_gemm_rowwise_grouped: eager_fp8_gemm_rowwise_grouped
fused_linear_cross_entropy: torch_lm_head_ce
fused_linear_jsd: torch_lm_head_jsd
gather_gemv: eager_gather_gemv
gdn_fwd_h: eager
gdpa: eager_gdpa
geglu: torch_geglu
gemm: aten_matmul
grouped_gemm: aten_grouped_mm
int4_gemm: eager_int4_gemm
jagged_layer_norm: torch_jagged_layer_norm_unbind_torch_layer_norm
jagged_mean: torch_jagged_mean_torch_sum
jagged_softmax: torch_jagged_softmax_unbind_torch_softmax
jagged_sum: torch_jagged_sum_no_pad
jsd: torch_jsd
kl_div: torch_kl_div
launch_latency: nop_python_function
layer_norm: torch_layer_norm
low_mem_dropout: eager_dropout
mamba2_chunk_scan: eager
mamba2_chunk_state: eager
mixed_gemm: aten_bf16_bf16
mx4_to_fp32: fbgemm_mx4_to_fp32
ragged_attention: hstu
rms_norm: llama_rms
rope: apply_rotary_pos_emb
softmax: naive_softmax
sum: torch_sum
swiglu: torch_swiglu
template_attention: test_no_exp2
vector_add: torch_add
vector_exp: torch_exp
welford: eager_layer_norm
