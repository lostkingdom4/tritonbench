name: linux-benchmark-mi350
on:
  workflow_call:
    secrets:
      TRITONBENCH_SCRIBE_GRAPHQL_ACCESS_TOKEN:
        required: True
        description: |
          Tritonbench Scribe Graph Access Token
    inputs:
      test_type:
        required: True
        type: string
        description: |
          Type of the test (single or abtest)
      benchmark_name:
        required: True
        type: string
        description: |
          Benchmark name
      venv_name:
        required: True
        default: "triton-main"
        type: string
        description: |
          Name of the virtual environment to use for the benchmark
      side_a_triton:
        type: string
        required: False
        default: "triton-lang/triton"
        description: |
          Triton repository to test on side A, e.g., "triton-lang/triton"
      side_a_commit:
        type: string
        required: False
        description: |
          Triton commit or tag to test on side A, e.g., "main"
      side_b_triton:
        type: string
        required: False
        default: "triton-lang/triton"
        description: |
          Triton repository to test on side B, e.g., "triton-lang/triton"
      side_b_commit:
        type: string
        required: False
        description: |
          Triton commit or tag to test on side B, e.g., "main"

jobs:
  linux-benchmark-mi350:
    if: github.repository_owner == 'meta-pytorch'
    runs-on: [amd-mi350-runner]
    timeout-minutes: 240
    environment: docker-s3-upload
    permissions:
      id-token: write
      contents: read
    env:
      WORKSPACE_DIR: "/workspace"
      UV_VENV_DIR: "/workspace/uv_venvs"
      SETUP_SCRIPT: "/workspace/setup_instance.sh"
      RUNNER_TYPE: "amd-mi350-runner"
      TRITONBENCH_SIDE_A_ENV: ${{ inputs.test_type == 'abtest' && 'triton-side-a' || inputs.venv_name }}
      TRITONBENCH_SIDE_B_ENV: "triton-side-b"
      JOB_NAME: tritonbench-mi350-benchmark-${{ inputs.venv_name }}-${{ inputs.test_type }}-${{ inputs.benchmark_name }}
      TRITONBENCH_SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.TRITONBENCH_SCRIBE_GRAPHQL_ACCESS_TOKEN }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    steps:
      - name: Checkout Tritonbench
        uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Authenticate with AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::308535385114:role/gha_workflow_upload-benchmark-results
          # The max duration enforced by the server side
          role-duration-seconds: 18000
          aws-region: us-east-1
      - name: Install Tritonbench env
        run: |
          set -eux
          if [[ "${TRITONBENCH_SIDE_A_ENV}" == "meta-triton" ]]; then
            CMD_SUFFIX=" --meta-triton"
          elif [[ "${TRITONBENCH_SIDE_A_ENV}" == "triton-main" ]]; then
            CMD_SUFFIX=" --triton-main"
          else
            CMD_SUFFIX=""
          fi
          bash ./.ci/tritonbench/setup-env.sh --hip ${CMD_SUFFIX}
      - name: Setup uploader dependencies
        run: |
          set -eux
          CONDA_ENV=${TRITONBENCH_SIDE_A_ENV} . "${SETUP_SCRIPT}"
          uv pip install -r .ci/upload/requirements.txt
      - name: Compile Triton side A (On Demand)
        if: ${{ inputs.side_a_triton && inputs.side_a_commit }}
        run: |
          set -eux
          bash ./.ci/triton/install.sh --repo ${{ inputs.side_a_triton }} --commit ${{ inputs.side_a_commit }} --side a
          export 'TRITONBENCH_SIDE_A_ENV="${{ env.TRITONBENCH_SIDE_A_ENV }}"' >> ${GITHUB_ENV}
      - name: Benchmark Triton (Side A)
        run: |
          set -eux
          CONDA_ENV=${TRITONBENCH_SIDE_A_ENV} . "${SETUP_SCRIPT}"
          bash .ci/tritonbench/run-benchmark.sh ${{ inputs.benchmark_name }} --conda-env ${TRITONBENCH_SIDE_A_ENV}
          mkdir -p benchmark-output
          cp -r ".benchmarks/${{ inputs.benchmark_name }}" benchmark-output/${TRITONBENCH_SIDE_A_ENV}
          sudo rm -rf .benchmarks || true
          # post-process result.json
          latest_result_json=$(find ./benchmark-output/${TRITONBENCH_SIDE_A_ENV} -name "result.json"  | sort -r | head -n 1)
          python3 ./.ci/test_infra/oss_ci_benchmark_v3.py --json ${latest_result_json} \
            --add-github-env --output ${latest_result_json}
      - name: Compile Triton side B (On Demand)
        if: ${{ inputs.test_type == 'abtest' && inputs.side_b_triton && inputs.side_b_commit }}
        run: |
          set -eux
          bash ./.ci/triton/install.sh --repo ${{ inputs.side_b_triton }} --commit ${{ inputs.side_b_commit }} --side b
          export 'TRITONBENCH_SIDE_B_ENV="${{ inputs.TRITONBENCH_SIDE_B_ENV }}"' >> ${GITHUB_ENV}
      - name: Benchmark Triton (Side B)
        if: ${{ inputs.test_type == 'abtest' && inputs.side_b_triton && inputs.side_b_commit }}
        run: |
          set -eux
          CONDA_ENV=${TRITONBENCH_SIDE_B_ENV} . "${SETUP_SCRIPT}"
          bash .ci/tritonbench/run-benchmark.sh ${{ inputs.benchmark_name }} --conda-env ${TRITONBENCH_SIDE_B_ENV}
          mkdir -p benchmark-output
          cp -r ".benchmarks/${{ inputs.benchmark_name }}" benchmark-output/${TRITONBENCH_SIDE_B_ENV}
          sudo rm -rf .benchmarks || true
          # post-process result.json
          latest_result_json=$(find ./benchmark-output/${TRITONBENCH_SIDE_B_ENV} -name "result.json"  | sort -r | head -n 1)
          python3 ./.ci/test_infra/oss_ci_benchmark_v3.py --json ${latest_result_json} \
            --add-github-env --output ${latest_result_json}
      - name: Upload result to GH Actions Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.JOB_NAME }}
          path: benchmark-output/
      - name: Upload result to Scribe
        run: |
          set -xeu
          CONDA_ENV=${TRITONBENCH_SIDE_A_ENV} . "${SETUP_SCRIPT}"
          if [[ -n "${TRITONBENCH_SIDE_A_ENV}" ]]; then
            latest_result_json=$(find ./benchmark-output/${TRITONBENCH_SIDE_A_ENV} -name "result.json"  | sort -r | head -n 1)
            python3 ./.ci/upload/scribe.py --json ${latest_result_json}
          fi
          if [[ -n "${TRITONBENCH_SIDE_B_ENV}" ]] && [[ -e "./benchmark-output/${TRITONBENCH_SIDE_B_ENV}" ]]; then
            latest_result_json=$(find ./benchmark-output/${TRITONBENCH_SIDE_B_ENV} -name "result.json"  | sort -r | head -n 1)
            python3 ./.ci/upload/scribe.py --json ${latest_result_json}
          fi
      - name: Rewrite Tritonbench json to ClickHouse style
        run: |
          set -xeu
          CONDA_ENV=${TRITONBENCH_SIDE_A_ENV} . "${SETUP_SCRIPT}"
          if [[ -n "${TRITONBENCH_SIDE_A_ENV}" ]]; then
            latest_result_json=$(find ./benchmark-output/${TRITONBENCH_SIDE_A_ENV} -name "result.json"  | sort -r | head -n 1)
            python3 ./.ci/test_infra/oss_ci_benchmark_v3.py --json ${latest_result_json} \
                  --output benchmark-output/clickhouse-results/result-${TRITONBENCH_SIDE_A_ENV}.json
          fi
          if [[ -n "${TRITONBENCH_SIDE_B_ENV}" ]] && [[ -e "./benchmark-output/${TRITONBENCH_SIDE_B_ENV}" ]]; then
            latest_result_json=$(find ./benchmark-output/${TRITONBENCH_SIDE_B_ENV} -name "result.json"  | sort -r | head -n 1)
            python3 ./.ci/test_infra/oss_ci_benchmark_v3.py --json ${latest_result_json} \
                  --output benchmark-output/clickhouse-results/result-${TRITONBENCH_SIDE_B_ENV}.json
          fi
      - name: Upload result to ClickHouse
        uses: pytorch/test-infra/.github/actions/upload-benchmark-results@main
        with:
          benchmark-results-dir: benchmark-output/clickhouse-results
          dry-run: false
          schema-version: v3
          github-token: ${{ secrets.GITHUB_TOKEN }}
